{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Model Representation\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "\n",
    "Learn the weights (w) and bias (b) that minimize the cost function.\n",
    "The cost is computed by summing over all training examples and is defined as:\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}  [\\log(\\hat{y}^{(i)}) + (1-y^{(i)} )  \\log(1-\\hat{y}^{(i)})]\\tag{3}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress = True,\n",
    "   formatter = {'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #Compute the sigmoid of z. z is a scalar or numpy array of any size\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.500000 0.880797]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Logistic Regression\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
    "\\frac{\\partial J}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)}) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "* $\\hat{y}^{(i)}$ is the model's prediction, while $y^{(i)}$ is the target value for the i-th example:\n",
    "\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create gradient descent function\n",
    "def gradient_descend(X, y, w, b, lr):\n",
    "    # Get the number of samples\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = sigmoid(y_pred)\n",
    "    #print (\"y_pred\", y_pred)\n",
    "\n",
    "    dw = np.dot(X.T, (y_pred-y)) / m\n",
    "    #print (\"dw\", dw)\n",
    "    db = np.sum(y_pred-y) / m\n",
    "    #print (\"db\", db)\n",
    "\n",
    "    # Make an update to the w parameter \n",
    "    w = w - (lr * dw)\n",
    "    b = b - (lr * db)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y, y_pred):\n",
    "    # Calculate log loss (cross-entropy loss)\n",
    "    epsilon = 1e-15  # Small value to avoid log(0)\n",
    "    m = y.shape[0]\n",
    "    return -1/m * np.sum(y*np.log(y_pred + epsilon) + (1-y)*np.log(1-y_pred + epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, lr = 0.01, n_iters = 1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    # Parameters\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    \n",
    "    # Iteratively make updates\n",
    "    for epoch in range(n_iters): \n",
    "        w, b = gradient_descend(X, y, w, b, lr)\n",
    "        # Debugging - Calculate the cost and print it every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            y_pred = np.dot(X, w) + b\n",
    "            y_pred = sigmoid(y_pred)\n",
    "            # compute cost\n",
    "            print(f'After {epoch} iterations the cost is {cost(y, y_pred)}')\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = sigmoid(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train - 2 samples [[12.880000 18.220000 84.450000 493.100000 0.121800 0.166100 0.048250\n",
      "  0.053030 0.170900 0.072530 0.442600 1.169000 3.176000 34.370000\n",
      "  0.005273 0.023290 0.014050 0.012440 0.018160 0.003299 15.050000\n",
      "  24.370000 99.310000 674.700000 0.145600 0.296100 0.124600 0.109600\n",
      "  0.258200 0.088930]\n",
      " [11.130000 22.440000 71.490000 378.400000 0.095660 0.081940 0.048240\n",
      "  0.022570 0.203000 0.065520 0.280000 1.467000 1.994000 17.850000\n",
      "  0.003495 0.030510 0.034450 0.010240 0.029120 0.004723 12.020000\n",
      "  28.260000 77.800000 436.600000 0.108700 0.178200 0.156400 0.064130\n",
      "  0.316900 0.080320]]\n",
      "y_train - 2 values [1 1]\n",
      "After 0 iterations the cost is 21.86190681699841\n",
      "After 100 iterations the cost is 21.85350900266123\n",
      "After 200 iterations the cost is 5.356133802990637\n",
      "After 300 iterations the cost is 6.491449801111514\n",
      "After 400 iterations the cost is 7.938331062826936\n",
      "After 500 iterations the cost is 3.4312611376375233\n",
      "After 600 iterations the cost is 3.8104807557117275\n",
      "After 700 iterations the cost is 4.824185958148375\n",
      "After 800 iterations the cost is 3.7017628417840944\n",
      "After 900 iterations the cost is 3.7318393247701707\n",
      "Weights:  [3.109961 4.236134 17.989036 7.150696 0.027656 -0.015727 -0.058963\n",
      " -0.024486 0.054091 0.022296 0.013122 0.291023 -0.123207 -8.183478\n",
      " 0.001368 -0.004703 -0.007558 -0.001041 0.005376 0.000152 3.293122\n",
      " 5.391520 18.048392 -9.779980 0.034174 -0.065649 -0.124391 -0.028732\n",
      " 0.076209 0.019391]\n",
      "Bias:  0.4048979144581369\n",
      "y_pred [1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
      "Accuracy:  0.9210526315789473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erradi\\AppData\\Local\\Temp\\ipykernel_8080\\3303058665.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Print 5 samples\n",
    "print(\"X_train - 2 samples\", X_train[:2])\n",
    "print(\"y_train - 2 values\", y_train[:2])\n",
    "\n",
    "w, b = fit(X_train, y_train, lr=0.01)\n",
    "y_pred = predict(X_test, w, b)\n",
    "# Convert the probabilities to binary values\n",
    "y_pred = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "acc = accuracy(y_pred, y_test)\n",
    "\n",
    "print(\"Weights: \", w)\n",
    "print(\"Bias: \", b)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"Accuracy: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmps460",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
