{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://github.com/maticvl/dataHacker/tree/master/pyTorch\" target=\"_parent\">This example extends the examples at this link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rte1f13L-qcW"
      },
      "source": [
        "### Computation graphs and Autograd in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sV_NWJXPS3O_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iX__NJIIF1a"
      },
      "source": [
        "## Simple example with gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw-BWzT_BlsQ"
      },
      "source": [
        "Let's now see a simple example of how the derivative is calculated. We will create a scalar tensor `x` and set the `requires_grad` parameter to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hOt6cjXjBmDb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x tensor(3., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor and set requires_grad=True to compute gradients\n",
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(\"x\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwk1w6AoVSk8"
      },
      "source": [
        "We will calculate `y` the following way:\\\n",
        "$ y = 3x^2 + 4x + 2$\n",
        "\n",
        "Now let's see what we get when we replace the `x` with our value `3`:  \n",
        "$ y = 3(3)^2 + 4(3) + 2 $\\\n",
        "$ y = 3*9 + 12 + 2$\\\n",
        "$ y = 27 + 12 + 2 $\\\n",
        "$ y = 41 $\n",
        "\\\n",
        "Now comes the part were we take the derivative of `y` with respect to the variable `x`.\\\n",
        "$\\frac{dy}{dx} = 2*3x + 4 = 6x + 4$\\\n",
        "If we replace `x` with our value `3`, we get the following:\n",
        "$6x + 4 = 6(3) + 4 = 18 + 4 = 22$\\\n",
        "So the gradient is equal to $22$\\\n",
        "Let's see how we can do this in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-DRxYu1cB7Zc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y = 3x^2 + 4x + 2 =  41.0\n"
          ]
        }
      ],
      "source": [
        "y = 3*x**2 + 4*x + 2\n",
        "print(\"y = 3x^2 + 4x + 2 = \", y.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call `y.backward()` to calculate the derivative for that function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NDV33SPOvR14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "derivative of `y` with respect to `x` = tensor(66.)\n"
          ]
        }
      ],
      "source": [
        "# Compute the derivative of `y` with respect to `x`\n",
        "y.backward()\n",
        "print(\"derivative of `y` with respect to `x` =\", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGA2TTZ2a72l"
      },
      "source": [
        "## Is there a way to turn off the gradient calculation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3myJit2O93s"
      },
      "source": [
        "The answer is yes, you can turn the gradient calculation anytime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xlL9kf-UbEaQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vcO3N1fMVYS0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = x.requires_grad_(False)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FSQmsP8mVgfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = x.detach()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFiwtMAiWX9_"
      },
      "source": [
        "# Gradient accumulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUcL2csoPiov"
      },
      "source": [
        "The auto gradient calculation does not reset the gradients automatically, therefore we have to reset them after each optimization. If we forget this step they could end up just accumulating.\\\n",
        "This sounds complex, but it is not, it's easy. To reset the gradients for a particular tensor, you can simply pass `x.grad.zero_()` and it will reset the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vxLVo9P_WS2G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(22.)\n",
            "tensor(22.)\n",
            "tensor(22.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  y = 3*x**2 + 4*x + 2\n",
        "  y.backward()\n",
        "\n",
        "  print(x.grad)\n",
        "  x.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Differentiation in Autograd\n",
        "Letâ€™s take a look at how autograd collects gradients. We create two tensors `a` and `b` with requires_grad=True. This signals to autograd that every operation on them should be tracked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create another tensor Q from a and b.\n",
        "Q = 3a^3 - b^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q =  tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
          ]
        }
      ],
      "source": [
        "Q = 3*a**3 - b**2\n",
        "print(\"Q = \", Q)"
      ]
    },
    {
      "attachments": {
        "image-2.png": {
          "image/png": "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"
        },
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Letâ€™s assume a and b to be parameters of an NN, and Q to be the error. In NN training, we want gradients of the error w.r.t. parameters, i.e.\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "When we call .backward() on Q, autograd calculates these gradients and stores them in the respective tensorsâ€™ .grad attribute.\n",
        "\n",
        "We need to explicitly pass a gradient argument in Q.backward() because it is a vector. gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t. itself, i.e.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like Q.sum().backward().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative of Q w.r.t a = 9*a**2 =  tensor([ 72., 162.])\n",
            "Derivative of Q w.r.t a = -2*b = tensor([-24., -16.])\n"
          ]
        }
      ],
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)\n",
        "\n",
        "# check if collected gradients are correct\n",
        "print(\"Derivative of Q w.r.t a = 9*a**2 = \", a.grad)\n",
        "print(\"Derivative of Q w.r.t a = -2*b =\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEO9m6PElhrw"
      },
      "source": [
        "## Optimizing parameters with autograd "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33aJwXNpQ6QJ"
      },
      "source": [
        "Create `x` tensor and compute `y` and `z` as follows:\\\n",
        "$y = 2x^2 + 3$\\\n",
        "$z = y^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TUZV2JHP1KIQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z =  tensor([25., 49., 81.], grad_fn=<PowBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1.,2.,3.], requires_grad=True)\n",
        "y = x * 2 + 3\n",
        "z = y ** 2\n",
        "print(\"z = \", z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZIGO5Md-4IED"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean tensor(51.6667, grad_fn=<MeanBackward0>)\n",
            "derivative tensor([ 6.6667,  9.3333, 12.0000])\n"
          ]
        }
      ],
      "source": [
        "out = z.mean()\n",
        "print(\"mean\", out)\n",
        "out.backward()\n",
        "print(\"derivative\", x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ED_QFLX_Ar5m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z =  tensor([25., 49., 81.], grad_fn=<PowBackward0>)\n",
            "tensor([20., 28., 36.])\n",
            "tensor([ 6.6667,  9.3333, 12.0000])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1.,2.,3.], requires_grad=True)\n",
        "y = x * 2 + 3\n",
        "z = y ** 2\n",
        "print(\"z = \", z)\n",
        "\n",
        "v = torch.tensor([1.,1.,1.])\n",
        "z.backward(v)\n",
        "print(x.grad)\n",
        "print(x.grad/len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-jlM9DcRE8N"
      },
      "source": [
        "Create a list for `w` and `b` in which we will save those values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGSWjSf4NIm8"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.,2.,3.,4.,5])\n",
        "y = x * 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY4ovV3UNo3S"
      },
      "outputs": [],
      "source": [
        "w_ = torch.tensor(5., requires_grad=True)\n",
        "b_ = torch.tensor(1., requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Wa9-7MNo6y"
      },
      "outputs": [],
      "source": [
        "for i_value in range(len(x)):\n",
        "  w = torch.tensor(5., requires_grad=True)\n",
        "  b = torch.tensor(1., requires_grad=True)\n",
        "\n",
        "  y_hat = w * x[i_value] + b\n",
        "  \n",
        "  error = y_hat - y[i_value]\n",
        "  loss = error ** 2 \n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  alpha = 0.01\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    w_.data -= alpha * (w.grad/len(x))\n",
        "    b_.data -= alpha * (b.grad/len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0crGHfd5RNNW"
      },
      "source": [
        "We will iterate and calculate the gradients for each value from the tensor `x_torch`. If we want to calculate the gradient for each number instead of the whole batch or list of numbers, we need to set the `w` and `b` parameters to the same number after calculation. After this step we do the forward pass for that number, and then we calculate the loss for that number and do the backpropagation. Update the `w` and `b` parameters and save them in the list we initialized earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSghVw7USY2q"
      },
      "source": [
        "To get the final gradient we need to sum all of the gradients for all the numbers and divide them with the total number of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9TIlFQfe47K"
      },
      "outputs": [],
      "source": [
        "print(w_)\n",
        "print(b_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEM3E6LMyQey"
      },
      "outputs": [],
      "source": [
        "w_ = torch.tensor(5., requires_grad=True)\n",
        "b_ = torch.tensor(1., requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7BuAfcFyW1N"
      },
      "outputs": [],
      "source": [
        "y_hat = w * x + b\n",
        "error = y_hat - y\n",
        "loss = error ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcPYM5nNyhFw"
      },
      "outputs": [],
      "source": [
        "loss=loss.mean()\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQCBKwfCysQX"
      },
      "outputs": [],
      "source": [
        "w.data -= alpha * w.grad\n",
        "b.data -= alpha * b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRi5BA-7SfyA"
      },
      "source": [
        "Let's now see how we can do the same thing but just with the whole set of numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1JgTpKleZec"
      },
      "outputs": [],
      "source": [
        "print(w)\n",
        "print(b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "004_autoGrad.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
