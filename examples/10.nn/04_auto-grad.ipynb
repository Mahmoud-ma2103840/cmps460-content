{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://github.com/maticvl/dataHacker/tree/master/pyTorch\" target=\"_parent\">This example extends the examples at this link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rte1f13L-qcW"
      },
      "source": [
        "### Computation graphs and Autograd in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sV_NWJXPS3O_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch's autograd (automatic differentiation) is a powerful tool that enables automatic computation of gradients for tensors. Gradients are essential for optimizing neural networks using techniques gradient descent. Autograd tracks the operations performed on tensors during forward pass and automatically computes the gradients of the output with respect to the input during the backward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iX__NJIIF1a"
      },
      "source": [
        "## Simple example with gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw-BWzT_BlsQ"
      },
      "source": [
        "Let's now see a simple example of how the derivative is calculated. We will create a scalar tensor `x` and set the `requires_grad` parameter to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hOt6cjXjBmDb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x tensor(3., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Define a tensor with requires_grad=True to enable tracking of operations for gradient computation\n",
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(\"x\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient of y with respect to x tensor(6.)\n"
          ]
        }
      ],
      "source": [
        "# Define a simple mathematical operation\n",
        "y = x ** 2  # y = x^2\n",
        "\n",
        "# Perform a backward pass to compute the gradient of y with respect to x\n",
        "# We call the backward() method on the output tensor (y or loss) \n",
        "# to initiate the computation of gradients.\n",
        "y.backward()\n",
        "\n",
        "# Access the gradient computed for x\n",
        "# The gradient represent the rate of change of the y with respect to x\n",
        "gradient = x.grad\n",
        "print(\"Gradient of y with respect to x\", gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwk1w6AoVSk8"
      },
      "source": [
        "We will calculate `y` the following way:\\\n",
        "$ y = 3x^2 + 4x + 2$\n",
        "\n",
        "Now let's see what we get when we replace the `x` with our value `3`:  \n",
        "$ y = 3(3)^2 + 4(3) + 2 $\\\n",
        "$ y = 3*9 + 12 + 2$\\\n",
        "$ y = 27 + 12 + 2 $\\\n",
        "$ y = 41 $\n",
        "\\\n",
        "Now comes the part were we take the derivative of `y` with respect to the variable `x`.\\\n",
        "$\\frac{dy}{dx} = 2*3x + 4 = 6x + 4$\\\n",
        "If we replace `x` with our value `3`, we get the following:\n",
        "$6x + 4 = 6(3) + 4 = 18 + 4 = 22$\\\n",
        "So the gradient is equal to $22$\\\n",
        "Let's see how we can do this in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-DRxYu1cB7Zc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y = 3x^2 + 4x + 2 =  41.0\n"
          ]
        }
      ],
      "source": [
        "y = 3*x**2 + 4*x + 2\n",
        "print(\"y = 3x^2 + 4x + 2 = \", y.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call `y.backward()` to calculate the derivative for that function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NDV33SPOvR14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "derivative of `y` with respect to `x` = tensor(28.)\n"
          ]
        }
      ],
      "source": [
        "# Compute the derivative of `y` with respect to `x`\n",
        "y.backward()\n",
        "print(\"derivative of `y` with respect to `x` =\", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGA2TTZ2a72l"
      },
      "source": [
        "### Is there a way to turn off the gradient calculation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3myJit2O93s"
      },
      "source": [
        "The answer is yes, you can turn the gradient calculation anytime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xlL9kf-UbEaQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vcO3N1fMVYS0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = x.requires_grad_(False)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FSQmsP8mVgfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = x.detach()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFiwtMAiWX9_"
      },
      "source": [
        "# Gradient accumulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUcL2csoPiov"
      },
      "source": [
        "The auto gradient calculation does not reset the gradients automatically, therefore we have to reset them after each optimization. If we forget this step they could end up just accumulating.\\\n",
        "This sounds complex, but it is not, it's easy. To reset the gradients for a particular tensor, you can simply pass `x.grad.zero_()` and it will reset the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vxLVo9P_WS2G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(22.)\n",
            "tensor(22.)\n",
            "tensor(22.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  y = 3*x**2 + 4*x + 2\n",
        "  y.backward()\n",
        "\n",
        "  print(x.grad)\n",
        "  x.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x1: tensor([2., 3.], requires_grad=True)\n",
            "x2: tensor([2., 1.], requires_grad=True)\n",
            "Gradients of loss with respect to w1: -16.0\n",
            "Gradients of loss with respect to w2: -8.0\n",
            "Gradient of loss with respect to b: -6.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define input features (x1 and x2)\n",
        "x1 = torch.tensor([2, 3], dtype=torch.float32, requires_grad=True)\n",
        "x2 = torch.tensor([2, 1], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "print(\"x1:\", x1)\n",
        "print(\"x2:\", x2)\n",
        "\n",
        "# Define weights and bias\n",
        "w1 = torch.tensor([3, 2], dtype=torch.float32, requires_grad=True)\n",
        "w2 = torch.tensor([1, 4], dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(1, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Define the forward pass of a simple neural network\n",
        "y_pred = x1 * w1 + x2 * w2 + b\n",
        "\n",
        "# Define a target value (ground truth)\n",
        "y_true = torch.tensor([11, 15], dtype=torch.float32)\n",
        "\n",
        "# Compute the loss manually\n",
        "loss = torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "# Compute gradients manually using chain rule\n",
        "dloss_dy_pred = 2 * (y_pred - y_true) / y_pred.numel()  # dloss/dy_pred\n",
        "dloss_dw1 = (dloss_dy_pred * x1).sum()  # dloss/dw1 = dloss/dy_pred * dy_pred/dw1 = dloss/dy_pred * x1\n",
        "dloss_dw2 = (dloss_dy_pred * x2).sum()  # dloss/dw2 = dloss/dy_pred * dy_pred/dw2 = dloss/dy_pred * x2\n",
        "dloss_db = dloss_dy_pred.sum()  # dloss/db = dloss/dy_pred * dy_pred/db = dloss/dy_pred * 1\n",
        "\n",
        "# Print the computed gradients\n",
        "print(\"Gradients of loss with respect to w1:\", dloss_dw1.item())\n",
        "print(\"Gradients of loss with respect to w2:\", dloss_dw2.item())\n",
        "print(\"Gradient of loss with respect to b:\", dloss_db.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Differentiation in Autograd\n",
        "Let's enhance the example with two input variables in the neural network. We'll use two input features (x1 and x2), two corresponding weights (w1 and w2), and one bias term (b). We'll perform a forward pass, compute the loss, and then compute the gradients of the loss with respect to the weights and bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient is defined as:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} ( \\hat{y}^{(i)} - y^{(i)})x^{(i)} \\tag{1}\\\\\n",
        "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)}) \\tag{2}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "Where:\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "* $\\hat{y}^{(i)} = f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient(X, y, y_pred): \n",
        "    m = X.shape[0]\n",
        "\n",
        "    dw = (2/m) * np.dot(X.T, (y_pred - y))\n",
        "    db = (2/m) * np.sum(y_pred - y)\n",
        "\n",
        "    return dw, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x1: tensor([2., 3.], requires_grad=True)\n",
            "x2: tensor([2., 1.], requires_grad=True)\n",
            "w1: tensor([3., 2.], requires_grad=True)\n",
            "w2: tensor([1., 4.], requires_grad=True)\n",
            "b: tensor(1., requires_grad=True)\n",
            "y_pred: tensor([ 9., 11.], grad_fn=<AddBackward0>)\n",
            "y_true: tensor([11., 15.])\n",
            "loss: tensor(10., grad_fn=<MeanBackward0>)\n",
            "Gradients of loss with respect to w1: tensor([ -4., -12.])\n",
            "Gradients of loss with respect to w2: tensor([-4., -4.])\n",
            "Gradient of loss with respect to b: tensor(-6.)\n",
            "dw: [-16.  -8.]\n",
            "db: -6.0\n",
            "dw: [-8. -4.]\n",
            "db: -3.0\n"
          ]
        }
      ],
      "source": [
        "# Define input features (x1 and x2) as tensors \n",
        "# and set requires_grad=True to compute gradients with respect to \n",
        "# x1 and x2 during the backward pass\n",
        "x1 = torch.tensor([2, 3], dtype=torch.float32, requires_grad=True)\n",
        "x2 = torch.tensor([2, 1], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "print(\"x1:\", x1)\n",
        "print(\"x2:\", x2)\n",
        "\n",
        "# Define weights and bias\n",
        "w1 = torch.tensor([3, 2], dtype=torch.float32, requires_grad=True)\n",
        "w2 = torch.tensor([1, 4], dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(1, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "print(\"w1:\", w1)\n",
        "print(\"w2:\", w2)\n",
        "print(\"b:\", b)\n",
        "\n",
        "# Define the forward pass of a simple neural network\n",
        "# to compute the predicted output y_pred using the defined weights, \n",
        "# input features, and bias term\n",
        "y_pred = x1 * w1 + x2 * w2 + b\n",
        "print(\"y_pred:\", y_pred)\n",
        "\n",
        "# Define a target value (ground truth)\n",
        "y_true = torch.tensor([11, 15], dtype=torch.float32)\n",
        "print(\"y_true:\", y_true)\n",
        "\n",
        "# Define a loss function (mean squared error)\n",
        "# between the predicted output y_pred and the target y_true\n",
        "loss = torch.mean((y_pred - y_true) ** 2)\n",
        "print(\"loss:\", loss)\n",
        "\n",
        "# Perform a backward pass to compute the gradients of loss with respect to \n",
        "# to compute the gradients of the loss with respect to the w1 and w2, and b\n",
        "loss.backward()\n",
        "\n",
        "# Access the gradients computed for w1, w2, and b\n",
        "# The gradients represent the rate of change of the loss with respect to w1, w2, and b\n",
        "gradient_w1 = w1.grad\n",
        "gradient_w2 = w2.grad\n",
        "gradient_b = b.grad\n",
        "\n",
        "# Print the gradients\n",
        "print(\"Gradients of loss with respect to w1:\", gradient_w1)\n",
        "print(\"Gradients of loss with respect to w2:\", gradient_w2)\n",
        "print(\"Gradient of loss with respect to b:\", gradient_b)\n",
        "\n",
        "X = np.array(list(zip(x1.detach().numpy(), x2.detach().numpy())))\n",
        "#print(\"X:\", X)\n",
        "w = np.array(list(zip(w1.detach().numpy(), w2.detach().numpy())))\n",
        "#print(\"w:\", w)\n",
        "y = y_true.detach().numpy()\n",
        "#print(\"y:\", y)\n",
        "y_pred = y_pred.detach().numpy()\n",
        "#print(\"y_pred:\", y_pred)\n",
        "b = b.detach().numpy()\n",
        "#print(\"b:\", b)\n",
        "dw, db = compute_gradient(X, y, y_pred)\n",
        "print(\"dw:\", dw)\n",
        "print(\"db:\", db)\n",
        "\n",
        "print(\"dw:\", dw/len(X))\n",
        "print(\"db:\", db/len(X))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "004_autoGrad.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
